# Code pour scraper le texte océrisé des documents de Retronews

La BnF a versé environ la moitié de ses documents de presse à sa filiale (à but lucratif) Retronews. Le Journal de Paris, par exemple, a disparu de Gallica mais est accessible [ici](https://www.retronews.fr/journal/journal-de-paris/14-juillet-1789/2969/4699172/1). C'est assez frustrant pour bon nombre de chercheurs, en particulier parce que Retronews ne dispose pas d'une API pour récupérer ce nom. C'est d'ailleurs probablement délibéré, puisque ces documents ne sont pas dans le domaine public. Pour réparer cette infamie, ce code vous permet de scraper le texte océrisé d'un document Retronews. Il repose sur Selenium (pas le choix...), d'où une certaine lenteur. Comptez une dizaine de secondes par document.

L'unique fonction du fichier python ici présent prend pour unique argument une URL. Elle peut soit être une URL Retronews (par exemple [https://www.retronews.fr/journal/journal-de-paris/14-juillet-1789/2969/4699172/1](https://www.retronews.fr/journal/journal-de-paris/14-juillet-1789/2969/4699172/1)), soit une URL Gallica qui redirige vers Retronews (par exemple [https://gallica.bnf.fr/ark:/12148/bpt6k37239760](https://gallica.bnf.fr/ark:/12148/bpt6k37239760)). La seconde option est commode si vous cherchez à scraper un journal entier : ces journaux sont toujours indexé dans Gallica, et donc accesibles par l'API. Pour le journal de Paris, vous pouvez par exemple lister tous les documents avec cette recherche API : [https://gallica.bnf.fr/SRU?operation=searchRetrieve&version=1.2&query=(dc.relation%20any%20cb327986698)%20and%20(ocr.quality%20all%20%22Texte%20disponible%22)&maximumRecords=50&startRecord=0&collapsing=false](xhttps://gallica.bnf.fr/SRU?operation=searchRetrieve&version=1.2&query=(dc.relation%20any%20cb327986698)%20and%20(ocr.quality%20all%20%22Texte%20disponible%22)&maximumRecords=50&startRecord=0&collapsing=false), en itérant bien sûr sur le paramètre `startRecord`, puis fournir toutes les URL des documents à notre belle fonction.

Si vous cherchez à optimiser ce code, vous pouvez le paralléliser. Quant à des recherches async, pour aller encore plus vite, je n'ai pas essayé mais je doute que ce soit une bonne idée : chaque requête charge une instance de firefox en arrière plan, la RAM risque donc de saturer assez vite.
